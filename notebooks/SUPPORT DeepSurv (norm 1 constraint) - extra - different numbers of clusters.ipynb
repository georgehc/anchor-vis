{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor Direction Visualization Framework: DeepSurv trained on the SUPPORT dataset (where the encoder has a Euclidean norm 1 constraint) -- using different numbers of clusters still with a mixture of von Mises-Fisher distributions\n",
    "\n",
    "This notebook is a modified and slightly shortened version of the main SUPPORT DeepSurv (norm 1 constraint) notebook, where the main difference is just that we try multiple numbers of clusters. We still cluster using a mixture of von Mises-Fisher distributions.\n",
    "\n",
    "*To save space, due to the supplemental file size constraint, this notebook needs to be re-run to see the code output.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in data (including some outputs from the already trained neural survival analysis model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "matplotlib.rcParams['font.family'] = ['sans-serif', 'Arial']\n",
    "matplotlib.rcParams['text.usetex'] = False\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'stixsans'\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "from visualization_utils import get_experiment_data, l2_normalize_rows, longest_common_prefix, \\\n",
    "    compute_median_survival_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explanation of the variables below:\n",
    "\n",
    "- `emb_direction`: these are embedding vectors for anchor direction estimation data\n",
    "- `emb_vis`: these are embedding vectors for the visualization raw inputs\n",
    "- `raw_direction`: raw inputs of the anchor direction estimation data\n",
    "- `raw_vis`: visualization raw inputs\n",
    "- `label_direction`: survival labels (2 columns: column 0 stores observed times and column 1 stores event indicators) of the anchor direction estimation data\n",
    "- `label_vis`: survival labels of the visualization raw inputs\n",
    "- `unique_train_times`: discretized time grid used for the predicted survival curves\n",
    "- `predicted_surv_vis`: predicted survival curves of the visualization raw inputs (so each visualization raw input has a predicted survival curve that is specified for the time grid given by `unique_train_times`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_direction, emb_vis, raw_direction, raw_vis, label_direction, label_vis, \\\n",
    "    _, unique_train_times, predicted_surv_vis \\\n",
    "        = get_experiment_data('support', '../train_models/output_tabular_hypersphere')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in this case, there are 665 anchor estimation data points and the embedding dimension is 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_direction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D PCA plot of the visualization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_surv_time_estimates = compute_median_survival_times(predicted_surv_vis, unique_train_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "emb_vis_pca_2d = pca.fit_transform(emb_vis)\n",
    "plt.axis('equal')\n",
    "plt.scatter(emb_vis_pca_2d[:, 0], emb_vis_pca_2d[:, 1], alpha=.6, c=median_surv_time_estimates, cmap='flare')\n",
    "plt.colorbar()\n",
    "plt.title('2D PCA plot of a DeepSurv embedding space (SUPPORT dataset)')\n",
    "# plt.savefig('support-embedding-space-pca-hypersphere.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D t-SNE plot of the visualization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=50, random_state=3676767249)\n",
    "emb_vis_tsne_2d = tsne.fit_transform(emb_vis)\n",
    "plt.axis('equal')\n",
    "plt.scatter(emb_vis_tsne_2d[:, 0], emb_vis_tsne_2d[:, 1], alpha=.6, c=median_surv_time_estimates, cmap='flare')\n",
    "plt.colorbar()\n",
    "plt.title('2D t-SNE plot of a DeepSurv embedding space (SUPPORT dataset)')\n",
    "# plt.savefig('support-embedding-space-tsne-hypersphere.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For different choices for the number of components, fit a mixture of von Mises-Fisher distributions\n",
    "\n",
    "We use the Expectation-Maximization algorithm implementation by Minyoung Kim: https://github.com/minyoungkim21/vmf-lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.statistics import pairwise_logrank_test\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import models\n",
    "import utils\n",
    "\n",
    "seed = 1861600023\n",
    "\n",
    "n_init = 100\n",
    "n_clusters_to_try = list(range(2, 11))\n",
    "logrank_p_values = []\n",
    "cluster_assignments = []\n",
    "cluster_models = []\n",
    "samples = torch.tensor(emb_direction, dtype=torch.float32)\n",
    "opts = {}\n",
    "opts['max_iters'] = 100  # maximum number of EM iterations\n",
    "opts['rll_tol'] = 1e-5  # tolerance of relative loglik improvement\n",
    "for n_clusters in n_clusters_to_try:\n",
    "    print('Trying %d clusters...' % n_clusters)\n",
    "\n",
    "    best_ll = -np.inf\n",
    "    best_cluster_assignment = None\n",
    "    for repeat_idx in range(n_init):\n",
    "        random.seed(seed + repeat_idx)\n",
    "        np.random.seed(seed + repeat_idx)\n",
    "        torch.manual_seed(seed + repeat_idx)\n",
    "        torch.cuda.manual_seed(seed + repeat_idx)\n",
    "        torch.cuda.manual_seed_all(seed + repeat_idx)\n",
    "\n",
    "        # create a model\n",
    "        mix = models.MixvMF(x_dim=emb_direction.shape[1], order=n_clusters)\n",
    "\n",
    "        # EM learning\n",
    "        ll_old = -np.inf\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for steps in range(opts['max_iters']):\n",
    "\n",
    "                # E-step\n",
    "                logalpha, mus, kappas = mix.get_params()\n",
    "                logliks, logpcs = mix(samples)\n",
    "                ll = logliks.sum()\n",
    "                jll = logalpha.unsqueeze(0) + logpcs\n",
    "                qz = jll.log_softmax(1).exp()\n",
    "\n",
    "                # tolerance check\n",
    "                if steps > 0:\n",
    "                    rll = (ll-ll_old).abs() / (ll_old.abs()+utils.realmin)\n",
    "                    if rll < opts['rll_tol']:\n",
    "                        break\n",
    "\n",
    "                ll_old = ll\n",
    "\n",
    "                # M-step\n",
    "                qzx = ( qz.unsqueeze(2) * samples.unsqueeze(1) ).sum(0)\n",
    "                qzx_norms = utils.norm(qzx, dim=1)\n",
    "                mus_new = qzx / qzx_norms\n",
    "                Rs = qzx_norms[:,0] / (qz.sum(0) + utils.realmin)\n",
    "                kappas_new = (mix.x_dim*Rs - Rs**3) / (1 - Rs**2 + 1e-6)\n",
    "                alpha_new = qz.sum(0) / samples.shape[0]\n",
    "\n",
    "                # assign new params\n",
    "                mix.set_params(alpha_new, mus_new, kappas_new)\n",
    "\n",
    "            logliks, logpcs = mix(samples)\n",
    "            ll = logliks.sum()\n",
    "            if ll > best_ll:\n",
    "                best_ll = ll\n",
    "                best_cluster_assignment = np.argmax(logpcs.numpy(), axis=1)\n",
    "\n",
    "    result = pairwise_logrank_test(label_direction[:, 0],\n",
    "                                   best_cluster_assignment,\n",
    "                                   label_direction[:, 1])\n",
    "    logrank_p_values.append(result.p_value)\n",
    "    cluster_assignments.append(best_cluster_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we make a violin plot for helping us select the number of clusters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 2.25))\n",
    "plt.violinplot(logrank_p_values, n_clusters_to_try)\n",
    "plt.xlabel('Number of clusters $k$')\n",
    "plt.ylabel('Log-rank test p-value')\n",
    "plt.yticks(np.linspace(0, 1, 11))\n",
    "# plt.savefig('support-logrank-pvalue-vs-nclusters-hypersphere.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using different clusters' anchor directions, make various visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that discretizes the all raw features (discrete and continuous) for raw feature probability heatmap\n",
    "# visualizations (this code is specific to the SUPPORT dataset but can be modified for other tabular data)\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder\n",
    "\n",
    "def transform(raw_features, continuous_n_bins=5):\n",
    "    feature_names = ['age', 'female', 'race', 'number of comorbidities',\n",
    "                     'diabetes', 'dementia', 'cancer', 'mean arterial blood pressure',\n",
    "                     'heart rate', 'respiration rate', 'temperature', 'white blood count',\n",
    "                     'serum sodium', 'serum creatinine']\n",
    "    binary_indices = [1, 4, 5]\n",
    "    continuous_indices = [0, 3, 7, 8, 9, 10, 11, 12, 13]\n",
    "    discretized_features = []\n",
    "    discretized_feature_names = []\n",
    "    all_n_bins_to_use = []\n",
    "    for idx in continuous_indices:\n",
    "        n_bins_to_use = continuous_n_bins\n",
    "        discretizer = KBinsDiscretizer(n_bins=n_bins_to_use,\n",
    "                                       strategy='quantile',\n",
    "                                       encode='onehot-dense')\n",
    "        new_features = discretizer.fit_transform(raw_features[:, idx].reshape(-1, 1).astype(float))\n",
    "        if discretizer.n_bins_[0] != n_bins_to_use:\n",
    "            n_bins_to_use = discretizer.n_bins_[0]\n",
    "\n",
    "        if n_bins_to_use > 1:\n",
    "            discretized_features.append(new_features)\n",
    "            for bin_idx in range(n_bins_to_use):\n",
    "                if bin_idx == 0:\n",
    "                    discretized_feature_names.append(feature_names[idx] + ' bin#1(-inf,%.2f)' % discretizer.bin_edges_[0][bin_idx+1])\n",
    "                elif bin_idx == n_bins_to_use - 1:\n",
    "                    discretized_feature_names.append(feature_names[idx] + ' bin#%d[%.2f,inf)' % (n_bins_to_use, discretizer.bin_edges_[0][bin_idx]))\n",
    "                else:\n",
    "                    # print(discretizer.bin_edges_[0][bin_idx:bin_idx+2])\n",
    "                    discretized_feature_names.append(feature_names[idx] + ' bin#%d[%.2f,%.2f)' % tuple([bin_idx + 1] + list(discretizer.bin_edges_[0][bin_idx:bin_idx+2])))\n",
    "        all_n_bins_to_use.append(n_bins_to_use)\n",
    "    for idx in binary_indices:\n",
    "        discretized_features.append(raw_features[:, idx].reshape(-1, 1).astype(float))\n",
    "        discretized_feature_names.append(feature_names[idx])\n",
    "        all_n_bins_to_use.append(1)\n",
    "\n",
    "    # race\n",
    "    discretizer = OneHotEncoder(sparse=False, categories=[[0, 1, 2, 3, 4, 5]])\n",
    "    discretized_features.append(discretizer.fit_transform(raw_features[:, 2].reshape(-1, 1).astype(float)))\n",
    "    discretized_feature_names.extend(['race cat#1(unspecified)',\n",
    "                                      'race cat#2(asian)',\n",
    "                                      'race cat#3(black)',\n",
    "                                      'race cat#4(hispanic)',\n",
    "                                      'race cat#5(other)',\n",
    "                                      'race cat#6(white)'])\n",
    "    all_n_bins_to_use.append(6)\n",
    "\n",
    "    # cancer\n",
    "    discretizer = OneHotEncoder(sparse=False, categories=[[0, 1, 2]])\n",
    "    discretized_features.append(discretizer.fit_transform(raw_features[:, 6].reshape(-1, 1).astype(float)))\n",
    "    discretized_feature_names.extend(['cancer cat#1(no)',\n",
    "                                      'cancer cat#2(yes)',\n",
    "                                      'cancer cat#3(metastatic)'])\n",
    "    all_n_bins_to_use.append(3)\n",
    "    return np.hstack(discretized_features), discretized_feature_names, all_n_bins_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vis_discretized, discretized_feature_names, all_n_bins_to_use = transform(raw_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .1\n",
    "\n",
    "for final_n_clusters in [3, 4, 5, 6, 7]:\n",
    "    print('[Number of clusters: %d]' % final_n_clusters)\n",
    "    final_cluster_assignment = cluster_assignments[n_clusters_to_try.index(final_n_clusters)]\n",
    "    center_of_mass = emb_direction.mean(axis=0)\n",
    "    anchor_directions = np.array([emb_direction[final_cluster_assignment == cluster_idx].mean(axis=0) - center_of_mass\n",
    "                                  for cluster_idx in range(final_n_clusters)])\n",
    "\n",
    "    all_raw_feature_probability_heatmaps = []\n",
    "    all_bin_edges = []\n",
    "\n",
    "    n_bins = 7\n",
    "    emb_vis_normalized = l2_normalize_rows(emb_vis - center_of_mass[np.newaxis, :])\n",
    "    anchor_directions_normalized = l2_normalize_rows(anchor_directions)\n",
    "    for cluster_idx in range(final_n_clusters):\n",
    "        projections = np.dot(emb_vis_normalized, anchor_directions_normalized[cluster_idx])\n",
    "\n",
    "        bin_counts, bin_edges = np.histogram(projections, bins=n_bins)\n",
    "        bin_edges_copy_with_inf_right_edge = bin_edges.copy()\n",
    "        bin_edges_copy_with_inf_right_edge[-1] = np.inf\n",
    "\n",
    "        bin_assignments = np.digitize(projections, bin_edges_copy_with_inf_right_edge) - 1\n",
    "\n",
    "        heatmap = np.zeros((len(discretized_feature_names), n_bins))\n",
    "        for discretized_feature_idx in range(len(discretized_feature_names)):\n",
    "            for projection_bin_idx in range(n_bins):\n",
    "                heatmap[discretized_feature_idx, projection_bin_idx] = raw_vis_discretized[bin_assignments == projection_bin_idx][:, discretized_feature_idx].mean()\n",
    "\n",
    "        all_raw_feature_probability_heatmaps.append(heatmap)\n",
    "        all_bin_edges.append(bin_edges)\n",
    "\n",
    "        # compute ranking table\n",
    "        projection_bin_counts = np.array([(bin_assignments == bin_idx).sum() for bin_idx in range(n_bins)])\n",
    "        print('[Cluster %d]' % (cluster_idx + 1))\n",
    "        current_row = 0\n",
    "        variable_pval_pairs = []\n",
    "        for variable_idx in range(len(all_n_bins_to_use)):\n",
    "            n_bins_to_use = all_n_bins_to_use[variable_idx]\n",
    "            if n_bins_to_use >= 2:\n",
    "                prefix = \\\n",
    "                    longest_common_prefix([discretized_feature_names[idx]\n",
    "                                           for idx in range(current_row,\n",
    "                                                            current_row + n_bins_to_use)])\n",
    "                if prefix.endswith('(') or prefix.endswith('['):\n",
    "                    prefix = prefix[:-1]\n",
    "                if prefix.endswith(' cat#') or prefix.endswith(' bin#'):\n",
    "                    prefix = prefix[:-5]\n",
    "                prefix = prefix.strip()\n",
    "                res = scipy.stats.chi2_contingency(\n",
    "                    heatmap[current_row:(current_row + n_bins_to_use), :]\n",
    "                    * projection_bin_counts[np.newaxis, :])\n",
    "                variable_pval_pairs.append((prefix, res[1]))\n",
    "            else:\n",
    "                indicator_row = heatmap[current_row:(current_row + n_bins_to_use), :]\n",
    "                res = scipy.stats.chi2_contingency(\n",
    "                    np.array([[indicator_row, 1. - indicator_row]])\n",
    "                    * projection_bin_counts[np.newaxis, :])\n",
    "                # print(discretized_feature_names[current_row], res[1])\n",
    "                variable_pval_pairs.append((discretized_feature_names[current_row], res[1]))\n",
    "            current_row += n_bins_to_use\n",
    "        for idx, (variable_name, pval) in enumerate(sorted(variable_pval_pairs, key=lambda x: x[1])):\n",
    "            print(idx + 1, '-', variable_name, '-', pval)\n",
    "            # print('%d &' % (idx + 1), variable_name, (('& $%.2E' % pval).replace('E-', '\\\\times 10^{-') + '}$ \\\\\\\\').replace('^{-0', '^{-'))\n",
    "        print()\n",
    "\n",
    "    # plot all raw feature probability heatmaps together\n",
    "\n",
    "    n_bins = 7\n",
    "    # fig, axn = plt.subplots(1, final_n_clusters, sharex=False, sharey=True, figsize=(12.5, 15))\n",
    "    fig, axn = plt.subplots(1, final_n_clusters, sharex=False, sharey=True, figsize=(15, 15))\n",
    "    cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
    "\n",
    "    for cluster_idx in range(final_n_clusters):\n",
    "        ax = axn.flat[cluster_idx]\n",
    "        current_row = 0\n",
    "        for idx, count in enumerate(all_n_bins_to_use[:-1]):\n",
    "            current_row += count\n",
    "            ax.plot([0, n_bins], [current_row, current_row], 'black')\n",
    "        sns.heatmap(pd.DataFrame(all_raw_feature_probability_heatmaps[cluster_idx],\n",
    "                                 index=discretized_feature_names,\n",
    "                                 columns=['%.2f' % x for x in (all_bin_edges[cluster_idx][:-1]\n",
    "                                                               + all_bin_edges[cluster_idx][1:])/2]),\n",
    "                    ax=ax,\n",
    "                    cmap=sns.light_palette(\"#4a72ae\", reverse=False, as_cmap=True),\n",
    "                    vmin=0, vmax=1,\n",
    "                    cbar=(cluster_idx == 0),\n",
    "                    cbar_ax=None if (cluster_idx != 0) else cbar_ax)\n",
    "        ax.set_xlabel('Projection onto\\nanchor direction\\nfor cluster %d' % (cluster_idx + 1))\n",
    "    fig.tight_layout(rect=[0, 0, .9, 1], pad=1.)\n",
    "    # plt.savefig('support-raw-feature-prob-heatmaps-hypersphere.pdf', bbox_inches='tight')\n",
    "\n",
    "    # compute survival probability heatmaps\n",
    "\n",
    "    from scipy import interpolate\n",
    "\n",
    "    n_rows = 10\n",
    "    discrete_time_grid = np.linspace(unique_train_times.min(), unique_train_times.max(), n_rows)\n",
    "\n",
    "    all_survival_probability_heatmaps = []\n",
    "    all_projection_bin_surv_curves = []\n",
    "    for cluster_idx in range(final_n_clusters):\n",
    "        projections = np.dot(emb_vis_normalized, anchor_directions_normalized[cluster_idx])\n",
    "\n",
    "        bin_counts, bin_edges = np.histogram(projections, bins=n_bins)\n",
    "        bin_edges_copy_with_inf_right_edge = bin_edges.copy()\n",
    "        bin_edges_copy_with_inf_right_edge[-1] = np.inf\n",
    "\n",
    "        bin_assignments = np.digitize(projections, bin_edges_copy_with_inf_right_edge) - 1\n",
    "\n",
    "        heatmap = np.zeros((n_rows, n_bins))\n",
    "        projection_bin_surv_curves = []\n",
    "        for projection_bin_idx in range(n_bins):\n",
    "            projection_bin_surv = interpolate.interp1d(\n",
    "                unique_train_times,\n",
    "                predicted_surv_vis[bin_assignments == projection_bin_idx].mean(axis=0))\n",
    "            projection_bin_surv_curves.append(\n",
    "                predicted_surv_vis[bin_assignments == projection_bin_idx].mean(axis=0))\n",
    "            heatmap[:, projection_bin_idx] = projection_bin_surv(discrete_time_grid)[::-1]\n",
    "\n",
    "        all_survival_probability_heatmaps.append(heatmap)\n",
    "        all_projection_bin_surv_curves.append(projection_bin_surv_curves)\n",
    "\n",
    "    # plot all survival probability heatmaps\n",
    "\n",
    "    # fig, axn = plt.subplots(1, final_n_clusters, sharex=False, sharey=True, figsize=(12.5, 3.5))\n",
    "    fig, axn = plt.subplots(1, final_n_clusters, sharex=False, sharey=True, figsize=(15, 3.5))\n",
    "    cbar_ax = fig.add_axes([.91, .3, .03, .4])\n",
    "\n",
    "    for cluster_idx in range(final_n_clusters):\n",
    "        ax = axn.flat[cluster_idx]\n",
    "        sns.heatmap(\n",
    "            pd.DataFrame(all_survival_probability_heatmaps[cluster_idx],\n",
    "                         index=['%.1f' % x for x in discrete_time_grid[::-1]],\n",
    "                         columns=['%.2f' % x for x in (all_bin_edges[cluster_idx][:-1]\n",
    "                                                       + all_bin_edges[cluster_idx][1:])/2]),\n",
    "            cmap=sns.light_palette(\"#4a72ae\", reverse=False, as_cmap=True),\n",
    "            vmin=0, vmax=1, ax=ax,\n",
    "            cbar=(cluster_idx == 0),\n",
    "            cbar_ax=None if (cluster_idx != 0) else cbar_ax)\n",
    "        ax.set_xlabel('Projection onto\\nanchor direction\\nfor cluster %d' % (cluster_idx + 1))\n",
    "        if cluster_idx == 0:\n",
    "            ax.set_ylabel('Survival time (days)')\n",
    "    fig.tight_layout(rect=[0, 0, .9, 1], pad=1.5)\n",
    "    # plt.savefig('support-surv-prob-heatmaps-hypersphere.pdf', bbox_inches='tight')\n",
    "\n",
    "    # ranking anchor directions based on median survival time\n",
    "\n",
    "    # estimate a survival curve for the top alpha fraction of visualization data points per cluster/anchor direction\n",
    "    anchor_direction_median_survival_time_pairs = []\n",
    "    for cluster_idx in range(final_n_clusters):\n",
    "        projections = np.dot(emb_vis_normalized, anchor_directions_normalized[cluster_idx])\n",
    "        q_alpha = np.sort(projections)[int(np.ceil((1-alpha)*len(projections)))]\n",
    "\n",
    "        surv_curv_alpha = predicted_surv_vis[projections >= q_alpha].mean(axis=0)\n",
    "        median_surv_time_estimate = compute_median_survival_times(surv_curv_alpha, unique_train_times)\n",
    "        anchor_direction_median_survival_time_pairs.append((cluster_idx, median_surv_time_estimate))\n",
    "\n",
    "    # sort anchor directions by median survival time estimates\n",
    "    sorted_anchor_direction_median_survival_time_pairs = \\\n",
    "        sorted(anchor_direction_median_survival_time_pairs, key=lambda x: x[1])\n",
    "    for cluster_idx, median_survival_time in sorted_anchor_direction_median_survival_time_pairs:\n",
    "        print('Cluster', cluster_idx + 1, ': median survival time estimate', median_survival_time)\n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
